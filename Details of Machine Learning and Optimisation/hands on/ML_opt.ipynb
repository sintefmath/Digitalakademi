{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kvup0GKu2mXr"
   },
   "source": [
    "\n",
    "\n",
    "# Details of machine learning and optimization\n",
    "**Hands on: Optimising production of a hydro power plant**\n",
    "\n",
    "This is a Jupyter notebook. You can run the code in each cell with \"shift + enter\" (or the run button above). Think of it as a script with blocks that can be run independently.\n",
    "\n",
    "The intention with this notebook is for you to get familiar with some more advanced details of machine learning and an example of how to combine it with optimization. Focus on the concepts rather than the code. \n",
    "\n",
    "## Running locally\n",
    "In order to run the notebook on your local machine, you need python >3.7 and the packages described in the environment_minimal.yml. The environment.yml specify versions for windows and environment_mac.yml for mac. We recommend installing with mamba. You can let mamba resolve all dependencies by using the minimal file.\n",
    "\n",
    "## Running on Colab\n",
    "The notebook can be run on Google Colab. The first cells below show how to get access to datafiles and all the required packages that we have installed in a sharable folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T07:39:16.275839Z",
     "start_time": "2021-09-15T07:39:16.266614Z"
    },
    "id": "7WIXMsPSeAxd"
   },
   "outputs": [],
   "source": [
    "# The necessary files muest be uploaded manually\n",
    "# Required files are environment.yml, 2res6turbines.png and \n",
    "# plant_with_6_gen_tailrace_loss.h5, aux_functions.py\n",
    "#from google.colab import files\n",
    "#uploaded = files.upload()\n",
    "# Chose file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T07:39:17.028727Z",
     "start_time": "2021-09-15T07:39:17.026309Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nNI49NXnn9f1",
    "outputId": "2dd15f1d-0610-46f5-af57-21fed7818d1f"
   },
   "outputs": [],
   "source": [
    "# In order to run the optimization code we need to install condacolab\n",
    "# Unfortunately, this has to be done every time you restart the notebook. Takes approx a minute.\n",
    "#!pip install --target=$nb_path condacolab\n",
    "#import condacolab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T07:39:18.269400Z",
     "start_time": "2021-09-15T07:39:18.267059Z"
    },
    "id": "S0zFqn73_S17"
   },
   "outputs": [],
   "source": [
    "# And install an environment with all the necessary packages\n",
    "#condacolab.install()\n",
    "#!conda env update -n base -f environment.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4G7cx3FSn6x1"
   },
   "source": [
    "## Load the relevant python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T07:40:44.760616Z",
     "start_time": "2021-09-15T07:40:44.749709Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "JgbN4oYmn6x2",
    "outputId": "cfeb1ec3-bc4a-45d8-e871-b5088d4a3260"
   },
   "outputs": [],
   "source": [
    "# Ignore all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# numpy allows for efficient array operations\n",
    "import numpy as np\n",
    "\n",
    "# pandas is used to structure all data in data frames and do simple operations. Works well for datasets that are \n",
    "# sufficiently small they can be stored in memory.\n",
    "import pandas as pd \n",
    "\n",
    "# scikit-learn contains simple and efficient tools for data mining and data analysis\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler  # Feature scaling\n",
    "from sklearn.model_selection import train_test_split  # Splitting test and training data randomly\n",
    "from sklearn.model_selection import cross_val_score  # Model evaluation\n",
    "from sklearn.model_selection import RandomizedSearchCV  # Hyper parameter search\n",
    "from sklearn.metrics import mean_squared_error  # Metric\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# keras is a high-level interface for deep learning. We will use tensorflow as backend\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# Functions for this specific dataset. Feel free to have a look in the file aux_functions.py\n",
    "from aux_functions import *\n",
    "\n",
    "# We use Matplotlib pyplot for visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "\n",
    "# seaborn provide useful statistical tools and quick visualisation\n",
    "import seaborn as sns  \n",
    "\n",
    "# We will use IPopt later for optimization\n",
    "import cyipopt\n",
    "\n",
    "# To see time impact\n",
    "import time\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "tf.autograph.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ev7J2v8En6x4"
   },
   "source": [
    "## The modelling challenge\n",
    "Consider a hydro power plant with six generators each with a power output of ``P_i``. Each generator has a loss, $\\Delta h_i^{head}$, which is related to the individual production of all generators in a complicated way. The **challenge is to optimise the power production while minimising the loss in the system**.\n",
    "\n",
    "The strategy is to use simulated data to train a neural network to describe the relation between production and loss, and then use that in the optimisation.\n",
    "\n",
    "In the figure below and in the data file, ``h_r1`` is the water level in the input reservoir, ``hr_2`` is the water level of the output reservoir. Part of the loss is above each turbine, $\\Delta h_i^{head}$, and part of the loss is summed below the turbines $\\Delta h^{tail}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T07:40:24.118621Z",
     "start_time": "2021-09-15T07:40:24.102400Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "id": "toyTdBNCn6x5",
    "outputId": "bca7075c-5e68-40f9-d1a2-7ba86339c9e6"
   },
   "outputs": [],
   "source": [
    "Image(\"./2res6turbines.png\",  width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-02T08:18:09.066627Z",
     "start_time": "2019-09-02T08:18:09.063968Z"
    },
    "id": "Ucmoa8vgn6x7"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4v-SZ7-fn6x8"
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T07:40:51.941310Z",
     "start_time": "2021-09-15T07:40:50.057818Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QlvQbg2mn6x9",
    "outputId": "8f078d41-e3ec-4ceb-af3b-3d1c59468f99"
   },
   "outputs": [],
   "source": [
    "file = 'plant_with_6_gen_tailrace_loss.h5'\n",
    "data = load_data(file)\n",
    "print(data.columns)\n",
    "variables = ['h_r1', 'h_r2', 'P_0', 'loss_head_0', 'P_1', 'loss_head_1', 'P_2', 'loss_head_2', \n",
    "         'P_3', 'loss_head_3', 'P_4', 'loss_head_4', 'P_5', 'loss_head_5', 'loss_tail']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSB3-oMwn6x-"
   },
   "source": [
    "### Plotting the data\n",
    "Visualisation and checking of data is very important. Always.\n",
    "\n",
    "*Visualise the data to check for outliers, faulty data, obvious parameter relations etc. Use jmin and jmax to zoom in on a smaller range of timesteps (jmin/jmax are bin/pixel numbers)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T07:40:57.635021Z",
     "start_time": "2021-09-15T07:40:56.125393Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-5HYB_Asn6x_",
    "outputId": "f6eab42f-25b9-41fb-ad88-29f329ba50dc",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Nplots = len(variables)\n",
    "jmin = 0      # minimum pixel number\n",
    "jmax = -1      # maximum pixel number, set to -1 to include all\n",
    "fig, axs = plt.subplots(Nplots, 1, figsize=(11,2.5*Nplots))\n",
    "\n",
    "for i in range(Nplots):\n",
    "    axs[i].plot(data[variables[i]].iloc[jmin:jmax])\n",
    "    axs[i].set_ylabel(variables[i])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAW1lNF8n6x_"
   },
   "source": [
    "### Additional visualisation and correlation\n",
    "Normally we would always use a scatter plot and correlation matrix to check for dependencies and feature engineering, but in order to get to more interesting aspects, **today we will skip those steps**. The code is given below for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T14:26:51.964361Z",
     "start_time": "2021-03-03T14:26:51.962272Z"
    },
    "id": "MhGhmXTLn6yA"
   },
   "outputs": [],
   "source": [
    "# Visualising data dependencies with seaborn, might take a while to run \n",
    "# Create the pairgrid object\n",
    "#grid = sns.PairGrid(data=data, vars=variables, diag_sharey=False)\n",
    "\n",
    "# Upper is a scatter plot\n",
    "#grid.map_lower(plt.scatter, alpha=0.8, s=20)\n",
    "\n",
    "# Diagonal is a histogram\n",
    "#grid.map_diag(sns.kdeplot)\n",
    "# Bottom is density plot\n",
    "#grid.map_upper(sns.kdeplot)\n",
    "#plt.savefig('/gridmap.pdf')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgOqCZ7dn6yB"
   },
   "source": [
    "Parameter correlation is a quick way to check which parameters are important for the analysis. A trained eye will spot the correlations from the gridplot above, but still a quantification can be useful. We use a Spearman correlation since we don't know if the relationship between parameters is linear (Pearson correlation assumes linearity), and we're not interested in absolute values of correlations but rather the relative correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T14:26:51.971456Z",
     "start_time": "2021-03-03T14:26:51.966326Z"
    },
    "id": "BU4UVu5Xn6yB"
   },
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "#corr_matrix = data[variables].corr(method='spearman')\n",
    "# Plot\n",
    "#fig, axs = plt.subplots(1, 1, figsize=(10, 10))\n",
    "#sns.heatmap(abs(corr_matrix), annot=True, cmap = plt.cm.autumn_r, fmt='.2f', ax=axs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vk5u-bmn6yB"
   },
   "source": [
    "### Feature engineering and input/output features\n",
    "We will use the heights and the individual power productions as input features, and the losses as output features. Since we want to optimise on the total loss, we will add it as an additional feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T07:43:28.199730Z",
     "start_time": "2021-09-15T07:43:28.190982Z"
    },
    "id": "K3xF9sqxn6yC"
   },
   "outputs": [],
   "source": [
    "loss_list = ['loss_head_0', 'loss_head_1', 'loss_head_2', \n",
    "             'loss_head_3', 'loss_head_4', 'loss_head_5', 'loss_tail']\n",
    "data['loss_tot'] = data[loss_list].sum(axis=1)  # summing all losses in each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T07:43:28.660674Z",
     "start_time": "2021-09-15T07:43:28.654779Z"
    },
    "id": "VD_P1_DMn6yC"
   },
   "outputs": [],
   "source": [
    "X_feat = ['h_r1', 'h_r2', 'P_0', 'P_1', 'P_2', 'P_3', 'P_4', 'P_5']\n",
    "y_feat = ['loss_head_0', 'loss_head_1', 'loss_head_2', \n",
    "          'loss_head_3', 'loss_head_4', 'loss_head_5', 'loss_tail', 'loss_tot']\n",
    "features = X_feat + y_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4J3SpfJtn6yD"
   },
   "source": [
    "### Training and test data\n",
    "\n",
    "Before we can train a model, we need to split the data in a training sample and a test sample.\n",
    "\n",
    "We must also split out a sample to use for tuning the model. That is called the validation data. We want the validation to take place on chronological data rather than the randomized training data.\n",
    "\n",
    "*Chose the fraction of data to want to use for test and validation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T07:43:47.738495Z",
     "start_time": "2021-09-15T07:43:47.735661Z"
    },
    "id": "3cB1qd3en6yD"
   },
   "outputs": [],
   "source": [
    "train_frac = 0.2                # Fraction of total data for training\n",
    "test_frac = 1 - train_frac      # Reserved for testing. Not used in training\n",
    "val_frac = 0.2                  # Fraction of training data used during training for tuning of hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T07:43:48.111544Z",
     "start_time": "2021-09-15T07:43:48.106751Z"
    },
    "id": "zAI-wzTcn6yD"
   },
   "outputs": [],
   "source": [
    "train_split_index = int(train_frac*len(data))\n",
    "val_split_index = int(val_frac*train_split_index) \n",
    "# We split out the validation data rather than using the validation_split variable in keras, since we want the \n",
    "# validation to take place on chronological data rather than the randomized training data\n",
    "\n",
    "data_train = data.iloc[:train_split_index]\n",
    "data_test = data.iloc[train_split_index:]\n",
    "data_val = data.iloc[train_split_index-val_split_index:train_split_index]      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68WTwQNMn6yE"
   },
   "source": [
    "### Visualising the split (on the dependent features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T07:43:50.800453Z",
     "start_time": "2021-09-15T07:43:49.575653Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UAR1FQk4n6yE",
    "outputId": "6ff26509-d64e-420d-a0a6-a5813a0abe48"
   },
   "outputs": [],
   "source": [
    "Nplots = len(y_feat)\n",
    "fig, axs = plt.subplots(Nplots, 1, figsize=(11,3*Nplots), sharey=False, sharex=True)\n",
    "\n",
    "for i in range(Nplots):\n",
    "    axs[i].plot(data_train[y_feat[i]], label='Training data')\n",
    "    axs[i].plot(data_val[y_feat[i]], label='Validation data')\n",
    "    axs[i].plot(data_test[y_feat[i]], label='Test data')\n",
    "    axs[i].set_ylabel(f'loss {y_feat[i]} [m]')\n",
    "    axs[i].legend()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T07:43:50.809880Z",
     "start_time": "2021-09-15T07:43:50.802231Z"
    },
    "id": "b_XliUQun6yF"
   },
   "outputs": [],
   "source": [
    "X_train = data_train[X_feat]#.values\n",
    "y_train = data_train[y_feat]#.values\n",
    "X_val = data_val[X_feat]#.values\n",
    "y_val = data_val[y_feat]#.values\n",
    "X_test = data_test[X_feat]#.values\n",
    "y_test = data_test[y_feat]#.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T07:43:51.404763Z",
     "start_time": "2021-09-15T07:43:51.400927Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YUDfm3Qln6yG",
    "outputId": "73c10cc3-9c36-499e-c490-6cfe0514bbc6"
   },
   "outputs": [],
   "source": [
    "print(np.shape(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cvaiWqSwn6yG"
   },
   "source": [
    "### Feature scaling\n",
    "Not all features have the same scale: Some have values of the order of 1000s, and some are 0.1. In order to let them equally influence the model, we need to \"put everything on the same scale\". We can either scale everything to a fixed range of values (MinMaxScaler) or change the distribution to become a normalised Gaussian (StandardScaler).\n",
    "\n",
    "Depending on the sample size, the test data can either be scaled with their own scaling (for large samples), or with the training sample (small samples). What to chose depends on how you would treat the actual data you will later use with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T07:44:02.455201Z",
     "start_time": "2021-09-15T07:44:02.434668Z"
    },
    "id": "ekNZsrTGn6yG"
   },
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "sc_X = MinMaxScaler(feature_range=[0,1])\n",
    "sc_y = MinMaxScaler(feature_range=[0,1])\n",
    "X_val_scaled = pd.DataFrame(sc_X.fit_transform(X_val), columns=X_feat)\n",
    "y_val_scaled = pd.DataFrame(sc_y.fit_transform(y_val), columns=y_feat)\n",
    "X_train_scaled = pd.DataFrame(sc_X.fit_transform(X_train), columns=X_feat)\n",
    "y_train_scaled = pd.DataFrame(sc_y.fit_transform(y_train), columns=y_feat)\n",
    "X_test_scaled = pd.DataFrame(sc_X.transform(X_test), columns=X_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tx84z0qLn6yG"
   },
   "source": [
    "## Neural network\n",
    "<span style=\"color:orange\"> Stop and wait for further instructions. </span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgT6937gn6yH"
   },
   "source": [
    "### Defining the neural network\n",
    "We use the keras framework with the Tensorflow backend to define the network (if you have no idea what that sentece was about, don't worry). \n",
    "\n",
    "The neural network itself will take the X-features as input and provide the y-estimates as output, but we also wrap it in a function that takes the number of neurons in each layer as input. The last parameter is dropout fraction which we apply to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T07:44:09.748676Z",
     "start_time": "2021-09-15T07:44:09.743016Z"
    },
    "id": "l1Ma3vZkn6yH"
   },
   "outputs": [],
   "source": [
    "def createModel(number_neurons1=10, number_neurons2=6, number_neurons3=4, dropout_frac=0.1):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=number_neurons1, activation='relu', input_dim=X_train.shape[1]))\n",
    "    model.add(Dropout(dropout_frac))  # Preventing overfitting\n",
    "    model.add(Dense(units=number_neurons2, activation='relu'))\n",
    "    model.add(Dropout(dropout_frac))  # Preventing overfitting\n",
    "    model.add(Dense(units=number_neurons3, activation='relu'))\n",
    "    model.add(Dropout(dropout_frac))  # Preventing overfitting\n",
    "    model.add(Dense(units=y_train.shape[1], activation='linear'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', metrics=['accuracy'], optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aen6sudsn6yI"
   },
   "source": [
    "*Chose the size of each layer and the dropout fraction by changing the values of the function parameters in the function call.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T07:44:42.199269Z",
     "start_time": "2021-09-15T07:44:42.001757Z"
    },
    "id": "a0UDZH57n6yI"
   },
   "outputs": [],
   "source": [
    "model = createModel(number_neurons1=10, number_neurons2=6, number_neurons3=4, dropout_frac=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T07:45:25.364051Z",
     "start_time": "2021-09-15T07:45:25.360692Z"
    }
   },
   "source": [
    "You may get a warning about Tensorflow here. It can be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-Ndihhen6yI"
   },
   "source": [
    "### Visualisation of a neural network\n",
    "If you want to visualise the neural network, you can e.g. use http://alexlenail.me/NN-SVG/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1bZN8w5n6yJ"
   },
   "source": [
    "### Training the model\n",
    "This may take a couple of minutes if the network is large. The figure below shows the loss computed on the training sample and the validation sample. The loss is the quantity the neural network aims to minimize.\n",
    "\n",
    "The relevant hyper parameters are:\n",
    "\n",
    "Epochs: The number of times the weights in the network will be updated. The value should be large enough for the network to converge (minimize loss to a stable level). If you run the training again without redefining the network, the training will continue from the previous session.\n",
    "\n",
    "Batch size: Determines the number of samples for cross-validation. Generally you should use as small a batch size as patience/available computing time allows for.\n",
    "\n",
    "Callbacks: A smart way to plot the loss during training. Requires the aux_functions.py\n",
    "\n",
    "*Chose some values for the hyper parameters and train the network*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T07:46:06.622435Z",
     "start_time": "2021-09-15T07:45:56.834924Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "I7JdQ8Tln6yJ",
    "outputId": "e8e4ba52-8bea-44df-e1c9-cae9c26a3c54",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_history = model.fit(X_train_scaled, y_train_scaled, \n",
    "                             validation_data=(X_val_scaled, y_val_scaled), \n",
    "                             epochs=50,                  \n",
    "                             batch_size=100,    \n",
    "                             callbacks=[plot_losses])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbthmtUEn6yK"
   },
   "source": [
    "### Using the model to predict the test data\n",
    "Remember that we scaled the input and output data before training the model. Now we need to rescale the predictions before we compare to the actual values from the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T07:46:08.509392Z",
     "start_time": "2021-09-15T07:46:08.269309Z"
    },
    "id": "H4ssCdh-n6yK"
   },
   "outputs": [],
   "source": [
    "y_scaled = pd.DataFrame(model.predict(X_test_scaled), columns=y_feat)  # Predicting\n",
    "y_pred = pd.DataFrame(sc_y.inverse_transform(y_scaled), columns=y_feat)  # Rescaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJH3coIBn6yK"
   },
   "source": [
    "### Plotting the results\n",
    "Compare the predicted results with the test data y values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T07:46:16.601655Z",
     "start_time": "2021-09-15T07:46:15.468463Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5Ooie44Ln6yK",
    "outputId": "77169686-a785-42a5-be87-0f4134578c1a"
   },
   "outputs": [],
   "source": [
    "Nplots = len(y_feat)\n",
    "fig, axs = plt.subplots(Nplots, 1, figsize=(11,3*Nplots), sharey=False, sharex=True)\n",
    "\n",
    "for i in range(Nplots):\n",
    "    axs[i].plot(y_pred[y_feat[i]].values, label = 'Estimated', alpha=0.8) #  '.'\n",
    "    axs[i].plot(y_test[y_feat[i]].values, label = 'Actual test data', alpha=0.8)  #'.'\n",
    "    axs[i].set_ylabel(f'loss {y_feat[i]} [m]')\n",
    "    axs[i].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-YQKCOFn6yL"
   },
   "source": [
    "It seems that the network tends to underestimate higher value and overstimate lower values. This is typical of an error-minimisation/least-squares approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBb703ann6yL"
   },
   "source": [
    "## Optmisation with the neural network\n",
    "<span style=\"color:orange\"> Before you start this section, wait for further instructions. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze9Gm7Pzn6yL"
   },
   "source": [
    "### Get the gradients\n",
    "In order to optimise with the neural network, we need to get the gradients (derivatives) of the feature we would like to optimise, with respect to all the input features. We are here and in the following going to assume that `h_r1` and `h_r2` are constant, which is true enough if they are largely unaffected by what we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T07:46:43.246464Z",
     "start_time": "2021-09-15T07:46:43.234340Z"
    },
    "id": "sxpjLEs_n6yM"
   },
   "outputs": [],
   "source": [
    "# This function may give a warning. Just ignore.\n",
    "def get_gradients(model, inputs):\n",
    "    features = tf.convert_to_tensor(inputs)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(features)\n",
    "        predictions = model(features)\n",
    "    gradients = tape.jacobian(predictions, features)\n",
    "    return gradients\n",
    "\n",
    "# Utility for visualizing optimization results\n",
    "def plot_results(axs, init, found, obj):\n",
    "    bound_palette = sns.dark_palette('brown')\n",
    "    production_palette = sns.light_palette('green')\n",
    "    turbine_palette = sns.color_palette()\n",
    "    loss_palette = sns.light_palette('red')\n",
    "    time_indices = np.array(range(obj.rows))\n",
    "    \n",
    "    for j in range(obj.free_cols):\n",
    "        c = turbine_palette[j]\n",
    "        axs[0].plot(time_indices, found[:,j], label=f'Turbine {j}', c = c)\n",
    "        axs[0].plot(time_indices, init[:,j], c = c, linestyle = 'dotted')\n",
    "    axs[0].legend(loc='upper left')\n",
    "    axs[0].set_title('Production per turbine')\n",
    "    \n",
    "    # We will sum \n",
    "    axs[1].plot(time_indices, obj.lower_cons[:len(time_indices)], label='lower', c=bound_palette[0], linestyle='dotted')\n",
    "    axs[1].plot(time_indices, obj.upper_cons[:len(time_indices)], label='upper', c=bound_palette[-1], linestyle='dotted')\n",
    "    axs[1].plot(time_indices, init.sum(axis=1), label='orig prod', c=production_palette[2], linestyle='dashed')\n",
    "    axs[1].plot(time_indices, found.sum(axis=1), label='opt prod', c=production_palette[-1])\n",
    "    axs[1].legend(loc='upper left')\n",
    "    axs[1].set_title('Total production')\n",
    "    \n",
    "    #This is a bit awkward, but no matter. The last output feature is total loss as seen above\n",
    "    init_original_features = obj.to_internal(init.flatten())\n",
    "    found_original_features = obj.to_internal(found.flatten())\n",
    "    axs[2].plot(time_indices, obj.model(init_original_features).numpy()[:,-1], label ='orig loss', c = loss_palette[2], linestyle='dashed')\n",
    "    axs[2].plot(time_indices, obj.model(found_original_features).numpy()[:,-1], label = 'opt loss', c = loss_palette[-1])\n",
    "    axs[2].legend(loc='upper left')\n",
    "    axs[2].set_title('Total loss')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PIsSmTrn6yM"
   },
   "source": [
    "### Simple optimization\n",
    "\n",
    "The full code appears here, but as can be seen it is a little complex. The important part to understand is that we provide ipyopt with the objective and constraints, as well as their partial derivatives. \n",
    "\n",
    "<span style=\"color:orange\"> For now, just execute the code... </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T07:46:49.177824Z",
     "start_time": "2021-09-15T07:46:49.168580Z"
    },
    "id": "Gmq_qHY-n6yM"
   },
   "outputs": [],
   "source": [
    "# We will be optimizing the pressures, but not the reservoir heights. They\n",
    "# are assumed to be unaffected.\n",
    "\n",
    "fixed_heights = X_test_scaled.values[:,:2]\n",
    "initial_values = X_test_scaled.values[:,2:]\n",
    "\n",
    "#We will also implement a basic class for this stuff\n",
    "#Subclass must define constraints, jacobian, lower_bound, upper_bound, lower_cons, upper_cons, num_cons\n",
    "class ModelBasedObjective(object):\n",
    "    \n",
    "    # Writing a more general handling of fixed columns is easy, but\n",
    "    # ultimately unnecessary for us\n",
    "    def __init__(self, rows, model, fixed, callback):\n",
    "        self.rows = rows\n",
    "        self.fixed_cols = 2\n",
    "        self.free_cols = 6\n",
    "        self.num_vars = self.rows * self.free_cols\n",
    "        self.model = model\n",
    "        self.fixed = fixed\n",
    "        self.intermediate=callback\n",
    "        \n",
    "    def to_internal(self, xk):\n",
    "        xk_shaped = xk.reshape(self.rows, self.free_cols)\n",
    "        fixed = self.fixed[:self.rows,:]\n",
    "        return np.concatenate((fixed, xk_shaped), axis=1)\n",
    "    \n",
    "    #Default adaptations if needed\n",
    "    def objective(self, xk):\n",
    "        return self.original_objective(self.to_internal(xk))\n",
    "    \n",
    "    def gradient(self, xk):\n",
    "        return self._gradient(self.to_internal(xk))\n",
    "    \n",
    "    # Objective implementation\n",
    "    def original_objective(self, x):\n",
    "        objval = self.model(x).numpy()[:,-1].sum()\n",
    "        return objval\n",
    "    \n",
    "    # Gradient implementation\n",
    "    def _gradient(self, x):\n",
    "        jacs = get_gradients(self.model, x)\n",
    "        comb = jacs.numpy()[:,-1].sum(axis=0)[:,self.fixed_cols:]\n",
    "        return comb.flatten()\n",
    "    \n",
    "def basic_callback(alg_mod, iter_count, obj_value, inf_pr, inf_du, mu, d_norm, regularization_size, alpha_du, alpha_pr, ls_trials):\n",
    "        if iter_count % 10 == 0:\n",
    "            print(f'Iteration {iter_count}, val={obj_value}. inf_pr, inf_du, mu: {inf_pr:.2f}, {inf_du:.2f}, {mu:.2f}')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T07:46:52.198169Z",
     "start_time": "2021-09-15T07:46:52.190236Z"
    },
    "id": "mjQZx4FVn6yN"
   },
   "outputs": [],
   "source": [
    "# Define a simple problem\n",
    "\n",
    "class SimpleProblemSparse(ModelBasedObjective):\n",
    "    \n",
    "    def __init__(self, rows, model, fixed, callback, prod_lower, prod_upper):\n",
    "        super().__init__(rows, model, fixed, callback)\n",
    "        \n",
    "        # Each constraint sums a single row of the x interpreted in 2d,\n",
    "        # so each gradient is zero outside of a single row, but then\n",
    "        # everything needs to be flattened to work with cyipopt\n",
    "        #We also need to set the bounds\n",
    "        self.lower_bound = np.array([0.0] * self.num_vars)\n",
    "        self.upper_bound = np.array([1.0] * self.num_vars)\n",
    "    \n",
    "        self.lower_cons = np.array([prod_lower] * rows, dtype=np.float)\n",
    "        self.upper_cons = np.array([prod_upper] * rows, dtype=np.float)\n",
    "        self.num_cons = self.rows\n",
    "        \n",
    "        lx = []\n",
    "        ly = []\n",
    "        for i in range(self.rows):\n",
    "            for j in range(self.free_cols):\n",
    "                lx.append(i)\n",
    "                ly.append(i * self.free_cols + j)\n",
    "\n",
    "        self.jac_sparsity_indices = (np.array(lx), np.array(ly))\n",
    "        \n",
    "        self.jac = 1.0*np.ones(self.num_vars)\n",
    "    \n",
    "    def constraints(self, xk):\n",
    "        x = xk.reshape(self.rows, self.free_cols)\n",
    "        return x.sum(axis=1)\n",
    "    \n",
    "    def jacobian(self, xk):\n",
    "        return self.jac\n",
    "    \n",
    "    def jacobianstructure(self):\n",
    "        return self.jac_sparsity_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T07:46:53.252612Z",
     "start_time": "2021-09-15T07:46:53.247127Z"
    },
    "id": "fDUN0hHkn6yN"
   },
   "outputs": [],
   "source": [
    "def optimize_model(obj, x0):  \n",
    "    \n",
    "    orig_obj = obj.objective(x0.flatten())\n",
    "    \n",
    "    print('Setting problem...')\n",
    "    \n",
    "    nlp = cyipopt.Problem(\n",
    "                n = obj.num_vars,\n",
    "                m = obj.num_cons,\n",
    "                problem_obj = obj,\n",
    "                lb = obj.lower_bound,\n",
    "                ub = obj.upper_bound,\n",
    "                cl = obj.lower_cons,\n",
    "                cu = obj.upper_cons,\n",
    "    )\n",
    "    \n",
    "    nlp.add_option('tol', 1e-2)\n",
    "    print('Solving...')\n",
    "\n",
    "    start = time.time()\n",
    "    x, info = nlp.solve(x0.flatten())\n",
    "    end = time.time()\n",
    "    \n",
    "    print(f'Initial value: {orig_obj}')\n",
    "    print(f'Found value: {info[\"obj_val\"]}')\n",
    "    print(f'Status: {info[\"status\"]}')\n",
    "    print(f'Elapsed time: {end - start:.2f} seconds')\n",
    "    \n",
    "    #Reconstruct x with correct columns\n",
    "    return x.reshape(obj.rows, obj.free_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dX-u1l4Xn6yN"
   },
   "source": [
    "*Try to change the lower or upper production values.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T07:47:59.505017Z",
     "start_time": "2021-09-15T07:47:57.812528Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "lnVyxtLLn6yO",
    "outputId": "8d0999c2-7d5f-4259-df70-7d0b848fa8e3"
   },
   "outputs": [],
   "source": [
    "# Let's try with only three time steps first\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR) #This line just suppress a warning in the output\n",
    "\n",
    "rows = 3\n",
    "init = initial_values[:rows,:]\n",
    "\n",
    "obj = SimpleProblemSparse(rows, model, fixed_heights, basic_callback, 1.0, 5.0)\n",
    "found = optimize_model(obj, init)\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(12,4))\n",
    "plot_results(ax, init, found, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T07:48:07.920794Z",
     "start_time": "2021-09-15T07:48:05.889664Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "j36d3xK8n6yO",
    "outputId": "6e08f420-0628-474c-c531-faafeab257e7"
   },
   "outputs": [],
   "source": [
    "#Reduce upper\n",
    "rows = 3\n",
    "init = initial_values[:rows,:]\n",
    "\n",
    "obj = SimpleProblemSparse(rows, model, fixed_heights, basic_callback, 1.0, 3.0)\n",
    "found = optimize_model(obj, init)\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(12,4))\n",
    "plot_results(ax, init, found, obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BibDX6zzn6yO"
   },
   "source": [
    "*For a more interesting case, try with 60 time steps.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xvtX3xuFn6yP"
   },
   "outputs": [],
   "source": [
    "rows = 60\n",
    "init = initial_values[:rows,:]\n",
    "\n",
    "obj = SimpleProblemSparse(rows, model, fixed_heights, basic_callback, 1.0, 3.0)\n",
    "found = optimize_model(obj, init)\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(12,4))\n",
    "plot_results(ax, init, found, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T07:48:55.585492Z",
     "start_time": "2021-09-15T07:48:28.634869Z"
    },
    "id": "9mEYgjw5n6yP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Same with sparsity\n",
    "\n",
    "rows = 60\n",
    "init = 0.2*np.ones((60,6))\n",
    "\n",
    "obj = SimpleProblemSparse(rows, model, fixed_heights, basic_callback, 1.0, 3.0)\n",
    "found = optimize_model(obj, init)\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(12,4))\n",
    "plot_results(ax, init, found, obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TC3yx_SMn6yQ"
   },
   "source": [
    "### With more realism\n",
    "\n",
    "The model above is not realistic, for many reasons. We wish to extend the problem definition somewhat. Let's say we have a degree of flexibility in our production, so that we are rather trying to match the total historical production over the time frame. The problem is then to minimize the total loss, while producing at least this much (We will ignore time correction).\n",
    "\n",
    "We can also add step constraints, and a time-varying production bound, but this will cause the model to be more complicated.\n",
    "\n",
    "*For now, just execute the code...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T07:48:55.591467Z",
     "start_time": "2021-09-15T07:48:55.588684Z"
    },
    "id": "zYkZGylin6yQ"
   },
   "outputs": [],
   "source": [
    "# We will need to compute the historical production.\n",
    "original_prod_per_time = initial_values.sum(axis=1)\n",
    "# Now, for the first N time steps, the total production is original_prod_per_time[:N].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T07:48:55.604516Z",
     "start_time": "2021-09-15T07:48:55.593621Z"
    },
    "id": "ygJD3JSYn6yR"
   },
   "outputs": [],
   "source": [
    "# Utility for making sinusoidal production\n",
    "def make_lower_prod(bottom, top, steps):\n",
    "    args = np.arange(0,2*np.pi, step = 2*np.pi/steps)\n",
    "    prod = ((1 - np.cos(args))/2.0) * (top - bottom) + bottom\n",
    "    return prod\n",
    "\n",
    "class TotalProductionBoundSparse(ModelBasedObjective):\n",
    "    \n",
    "    def __init__(self, rows, model, fixed, callback, prod_lower, prod_upper, total_prod):\n",
    "        super().__init__(rows, model, fixed, callback)\n",
    "        \n",
    "        self.lower_bound = np.array([0.0] * self.num_vars)\n",
    "        self.upper_bound = np.array([1.0] * self.num_vars)\n",
    "    \n",
    "        self.lower_cons = np.array([prod_lower] * rows + [total_prod], dtype=np.float)\n",
    "        self.upper_cons = np.array([prod_upper] * rows + [self.free_cols * self.rows], dtype=np.float)\n",
    "        self.num_cons = self.rows + 1\n",
    "        \n",
    "        lx = []\n",
    "        ly = []\n",
    "        for i in range(self.rows):\n",
    "            for j in range(self.free_cols):\n",
    "                lx.append(i)\n",
    "                ly.append(i * self.free_cols + j)\n",
    "        # Now append the last row, which has an index for every variable\n",
    "        for i in range(self.rows):\n",
    "            for j in range(self.free_cols):\n",
    "                lx.append(self.rows)\n",
    "                ly.append(i * self.free_cols + j)\n",
    "\n",
    "        self.jac_sparsity_indices = (np.array(lx), np.array(ly))\n",
    "        self.jac = np.concatenate((1.0*np.ones(self.num_vars), 1.0*np.ones(self.num_vars)))\n",
    "    \n",
    "    def constraints(self, xk):\n",
    "        x = xk.reshape(self.rows, self.free_cols)\n",
    "        return np.concatenate((x.sum(axis=1), [x.sum()]))\n",
    "    \n",
    "    def jacobian(self, xk):\n",
    "        return self.jac\n",
    "    \n",
    "    def jacobianstructure(self):\n",
    "        return self.jac_sparsity_indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kb4xgYr5n6yS"
   },
   "source": [
    "*Experiment with different conditions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T07:50:30.374113Z",
     "start_time": "2021-09-15T07:48:59.338182Z"
    },
    "id": "de_NNjiyn6yS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Strict curve\n",
    "rows = 60\n",
    "init = initial_values[:rows,:]\n",
    "\n",
    "obj = TotalProductionBoundSparse(rows, model, fixed_heights, basic_callback, 0.0, 6.0, original_prod_per_time[:rows].sum())\n",
    "found = optimize_model(obj, init)\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(12,4))\n",
    "plot_results(ax, init, found, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T07:50:38.187048Z",
     "start_time": "2021-09-15T07:50:30.377393Z"
    },
    "id": "myV-MKFmn6yS"
   },
   "outputs": [],
   "source": [
    "rows = 60\n",
    "init = 0.4* np.ones((rows, 6))\n",
    "\n",
    "obj = TotalProductionBoundSparse(rows, model, fixed_heights, basic_callback, 0.0, 6.0, original_prod_per_time[:rows].sum())\n",
    "found = optimize_model(obj, init)\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(12,4))\n",
    "plot_results(ax, init, found, obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T07:50:38.195718Z",
     "start_time": "2021-09-15T07:50:38.190036Z"
    }
   },
   "source": [
    "*Post your best set of conditions on Teams together with the resulting plot*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T07:52:58.617466Z",
     "start_time": "2021-09-15T07:52:58.609460Z"
    },
    "id": "JuRIacEBn6yT"
   },
   "outputs": [],
   "source": [
    "# Define a simple problem\n",
    "\n",
    "class TimeVaryingProductionBound(ModelBasedObjective):\n",
    "    \n",
    "    def __init__(self, rows, model, fixed, callback, prod_lower, prod_upper):\n",
    "        super().__init__(rows, model, fixed, callback)\n",
    "        \n",
    "        # Each constraint sums a single row of the x interpreted in 2d,\n",
    "        # so each gradient is zero outside of a single row, but then\n",
    "        # everything needs to be flattened to work with cyipopt\n",
    "        #We also need to set the bounds\n",
    "        self.lower_bound = np.array([0.0] * self.num_vars)\n",
    "        self.upper_bound = np.array([1.0] * self.num_vars)\n",
    "    \n",
    "        self.lower_cons = prod_lower\n",
    "        self.upper_cons = prod_upper\n",
    "        self.num_cons = self.rows\n",
    "        \n",
    "        lx = []\n",
    "        ly = []\n",
    "        for i in range(self.rows):\n",
    "            for j in range(self.free_cols):\n",
    "                lx.append(i)\n",
    "                ly.append(i * self.free_cols + j)\n",
    "\n",
    "        self.jac_sparsity_indices = (np.array(lx), np.array(ly))\n",
    "        \n",
    "        self.jac = 1.0*np.ones(self.num_vars)\n",
    "    \n",
    "    def constraints(self, xk):\n",
    "        x = xk.reshape(self.rows, self.free_cols)\n",
    "        return x.sum(axis=1)\n",
    "    \n",
    "    def jacobian(self, xk):\n",
    "        return self.jac\n",
    "    \n",
    "    def jacobianstructure(self):\n",
    "        return self.jac_sparsity_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T07:53:32.284481Z",
     "start_time": "2021-09-15T07:53:00.336336Z"
    },
    "id": "EMdHdL53n6yT"
   },
   "outputs": [],
   "source": [
    "# Historically accurate production\n",
    "rows = 60\n",
    "init = 0.4 * np.ones((rows,6))\n",
    "\n",
    "obj = TimeVaryingProductionBound(rows, model, fixed_heights, basic_callback, \n",
    "                                original_prod_per_time[:rows], [6.0] * rows)\n",
    "found = optimize_model(obj, init)\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(12,4))\n",
    "plot_results(ax, init, found, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G3zVafZkn6yT"
   },
   "outputs": [],
   "source": [
    "# Historically accurate production\n",
    "rows = 60\n",
    "init = initial_values[:rows,:]\n",
    "\n",
    "obj = TimeVaryingProductionBound(rows, model, fixed_heights, basic_callback, \n",
    "                                original_prod_per_time[:rows], [6.0] * rows)\n",
    "found = optimize_model(obj, init)\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(12,4))\n",
    "plot_results(ax, init, found, obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lly6Jwqfn6yU"
   },
   "source": [
    "## Evaluation of model performance\n",
    "<span style=\"color:orange\"> Before you start this section, wait for further instructions. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIfruTMjn6yU"
   },
   "source": [
    "### Evaluating the cross-validation scores\n",
    "One way to quantify if the neural network provides a reasonable fit is to compute the mean squared error on multiple splots of the test data (cross-validation). ScikitLearn has a function for that, which returns the score for each of the data splits. If the mean squared error is small, the model is a good fit to the data. If the variation is small, the model has also managed to generalise the information in the data. (For technical reasons ScikitLearn uses negative mean squared error).\n",
    "\n",
    "*Is your model good?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-15T07:57:32.548887Z",
     "start_time": "2021-09-15T07:57:28.372791Z"
    },
    "id": "xaP52zkgn6yU",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = cross_val_score(KerasRegressor(build_fn=createModel, nb_epoch=30, verbose=0), \n",
    "                         X_test_scaled, y_test,  cv=5, scoring=\"neg_mean_squared_error\")\n",
    "print(scores*-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5XrR9hUn6yU"
   },
   "source": [
    "## Hyperparameter optimization\n",
    "*Before you start this section, wait for further instructions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njwuJJJen6yV"
   },
   "source": [
    "### Manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfLQ2QEGn6yV"
   },
   "source": [
    "*Try different values for the hyper parameters by replacing the values below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T14:27:36.131176Z",
     "start_time": "2021-03-03T14:27:36.128458Z"
    },
    "id": "tpOs050jn6yV"
   },
   "outputs": [],
   "source": [
    "dropout_frac = 0      # fraction between 0 and 1\n",
    "number_neurons1 = 5   # integer\n",
    "number_neurons2 = 6   # integer\n",
    "number_neurons3 = 7   # integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T14:27:36.179800Z",
     "start_time": "2021-03-03T14:27:36.133612Z"
    },
    "id": "IUJ2KdKcn6yV"
   },
   "outputs": [],
   "source": [
    "model2 = createModel(dropout_frac=dropout_frac, \n",
    "                     number_neurons1=number_neurons1, \n",
    "                     number_neurons2=number_neurons2, \n",
    "                     number_neurons3=number_neurons3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JJluPX9n6yW"
   },
   "source": [
    "### Randomized search\n",
    "Manually searching through hyper parameters isn't very effective. Instead we will use an option called randomized search, where we specify some ranges for the hyper parameters.\n",
    "\n",
    "*Set some ranges for the layer sizes and search through the hyper parameter space. This may take a while, so we just run a very small grid with few iterations and cross-validations.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T14:27:36.510093Z",
     "start_time": "2021-03-03T14:27:36.503343Z"
    },
    "id": "2C9Jueqjn6yW"
   },
   "outputs": [],
   "source": [
    "param_dist = {\"number_neurons1\": randint(4, 6),\n",
    "              \"number_neurons2\": randint(7, 8),\n",
    "              \"number_neurons3\": randint(4, 6),\n",
    "              \"dropout_frac\": [0, 0.1, 0.2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T14:27:37.225805Z",
     "start_time": "2021-03-03T14:27:37.222742Z"
    },
    "id": "gnWuIk5yn6yW"
   },
   "outputs": [],
   "source": [
    "# In order to use randomized searh we need to define our model as a regressor\n",
    "k_model = KerasRegressor(build_fn=createModel, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T14:27:37.950775Z",
     "start_time": "2021-03-03T14:27:37.947682Z"
    },
    "id": "F6IolAu8n6yW"
   },
   "outputs": [],
   "source": [
    "# Define the search\n",
    "random_search = RandomizedSearchCV(k_model, param_distributions=param_dist,                   \n",
    "                                   scoring = \"neg_mean_squared_error\",\n",
    "                                   n_iter=1, cv=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T14:27:47.923656Z",
     "start_time": "2021-03-03T14:27:38.946021Z"
    },
    "id": "h13P8SYNn6yW",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run the search\n",
    "random_search.fit(X_train_scaled, y_train_scaled,\n",
    "                  validation_data=(X_val_scaled, y_val_scaled),\n",
    "                  epochs=30, batch_size=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T14:27:47.933212Z",
     "start_time": "2021-03-03T14:27:47.926586Z"
    },
    "id": "YEbtrW_Mn6yX"
   },
   "outputs": [],
   "source": [
    "# Print the best score from the random search\n",
    "random_search.best_score_*-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T12:03:04.585505Z",
     "start_time": "2019-12-05T12:03:04.581589Z"
    },
    "id": "7WgQRGd7n6yX"
   },
   "source": [
    "<span style=\"color:orange\"> Compare to the score you got for manual search </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T14:24:14.559001Z",
     "start_time": "2021-03-03T14:24:14.554803Z"
    },
    "id": "61R7RcyBn6yY"
   },
   "outputs": [],
   "source": [
    "# Print the best parameters\n",
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T12:03:42.046782Z",
     "start_time": "2019-12-05T12:03:42.042689Z"
    },
    "id": "AlYgBi2cn6yY"
   },
   "source": [
    "*Compare to the network sizes you had before. Is the fit better now?*\n",
    "\n",
    "In reality we would run a larger grid, so if you didn't get any improvement, you can try with \n",
    "\n",
    "best_params = {'dropout_frac': 0.1, 'number_neurons1': 18, 'number_neurons2': 10, 'number_neurons3': 12}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Share your best score and best_params on Teams*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0K-MbNCn6yY"
   },
   "source": [
    "### Visualising the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T14:25:14.147507Z",
     "start_time": "2021-03-03T14:25:13.943869Z"
    },
    "id": "ASlnM1XVn6yZ"
   },
   "outputs": [],
   "source": [
    "y_scaled = pd.DataFrame(random_search.predict(X_test_scaled), columns=y_feat)  # Predicting\n",
    "y_pred = pd.DataFrame(sc_y.inverse_transform(y_scaled), columns=y_feat)  # Rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T14:25:42.446055Z",
     "start_time": "2021-03-03T14:25:40.812295Z"
    },
    "id": "hQlOLdSEn6yZ"
   },
   "outputs": [],
   "source": [
    "Nplots = len(y_feat)\n",
    "fig, axs = plt.subplots(Nplots, 1, figsize=(11,3*Nplots), sharey=False, sharex=True)\n",
    "\n",
    "for i in range(Nplots):\n",
    "    axs[i].plot(y_pred[y_feat[i]].values, label = 'Estimated', alpha=0.8) \n",
    "    axs[i].plot(y_test[y_feat[i]].values, label = 'Actual test data', alpha=0.8)     \n",
    "    axs[i].set_ylabel(f'loss {y_feat[i]} [m]')\n",
    "    axs[i].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XnQUp7JFn6yZ"
   },
   "source": [
    "## Optimisation with better model\n",
    "\n",
    "Let's try this again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HWEA46GRn6ya"
   },
   "outputs": [],
   "source": [
    "# We must train a model with the best hyperparameters from the random search\n",
    "#model2 = createModel(dropout_frac=random_search.best_params_['dropout_frac'], \n",
    "#                     number_neurons1=random_search.best_params_['number_neurons1'],\n",
    "#                     number_neurons2=random_search.best_params_['number_neurons2'],\n",
    "#                     number_neurons3=random_search.best_params_['number_neurons3'])\n",
    "\n",
    "dropout_frac = 0.1      # fraction between 0 and 1\n",
    "number_neurons1 = 18   # integer\n",
    "number_neurons2 = 10   # integer\n",
    "number_neurons3 = 12   # integer\n",
    "\n",
    "model2 = createModel(dropout_frac=dropout_frac, \n",
    "                     number_neurons1=number_neurons1, \n",
    "                     number_neurons2=number_neurons2, \n",
    "                     number_neurons3=number_neurons3)\n",
    "\n",
    "training_history = model2.fit(X_train_scaled, y_train_scaled, \n",
    "                             validation_data=(X_val_scaled, y_val_scaled), \n",
    "                             epochs=200,                  \n",
    "                             batch_size=100,    \n",
    "                             callbacks=[plot_losses])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzU7B-njn6ya"
   },
   "source": [
    "*Try some of the same snippets as before. Are the results different?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FYqYeMfrn6ya"
   },
   "outputs": [],
   "source": [
    "# With total production bound\n",
    "\n",
    "rows = 60\n",
    "init = 0.4* np.ones((rows, 6))\n",
    "\n",
    "obj = TotalProductionBoundSparse(rows, model2, fixed_heights, basic_callback, 0.0, 6.0, original_prod_per_time[:rows].sum())\n",
    "found = optimize_model(obj, init)\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(12,4))\n",
    "plot_results(ax, init, found, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TeRoyf0Tn6yb"
   },
   "outputs": [],
   "source": [
    "# Historically accurate production, starting from 0.4\n",
    "rows = 60\n",
    "init = 0.4 * np.ones((rows,6))\n",
    "\n",
    "obj = TimeVaryingProductionBound(rows, model2, fixed_heights, basic_callback, \n",
    "                                original_prod_per_time[:rows], [6.0] * rows)\n",
    "found = optimize_model(obj, init)\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(12,4))\n",
    "plot_results(ax, init, found, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6zl05gEcn6yb"
   },
   "outputs": [],
   "source": [
    "# Historically accurate production, starting from history\n",
    "rows = 120\n",
    "init = initial_values[:rows,:]\n",
    "\n",
    "obj = TimeVaryingProductionBound(rows, model2, fixed_heights, basic_callback, \n",
    "                                original_prod_per_time[:rows], [6.0] * rows)\n",
    "found = optimize_model(obj, init)\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(12,4))\n",
    "plot_results(ax, init, found, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQEoSLqan6yc"
   },
   "outputs": [],
   "source": [
    "# Historically accurate production, starting from flat\n",
    "rows = 120\n",
    "init = 0.4 * np.ones((rows,6))\n",
    "\n",
    "obj = TimeVaryingProductionBound(rows, model2, fixed_heights, basic_callback, \n",
    "                                original_prod_per_time[:rows], [6.0] * rows)\n",
    "found = optimize_model(obj, init)\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(12,4))\n",
    "plot_results(ax, init, found, obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Share the final production plan on Teams and comment on your findings*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "4J3SpfJtn6yD",
    "68WTwQNMn6yE",
    "cvaiWqSwn6yG",
    "zgT6937gn6yH",
    "v-Ndihhen6yI",
    "A1bZN8w5n6yJ",
    "mbthmtUEn6yK",
    "LJH3coIBn6yK",
    "ze9Gm7Pzn6yL",
    "_PIsSmTrn6yM",
    "TC3yx_SMn6yQ",
    "LIfruTMjn6yU",
    "njwuJJJen6yV",
    "1JJluPX9n6yW",
    "Z0K-MbNCn6yY"
   ],
   "name": "ML_opt.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
